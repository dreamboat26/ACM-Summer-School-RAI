{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc1b0ba-2469-4ec6-821a-372139d83fac",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:60px;\"><center>Transformer</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eda700-90b7-41a6-85d3-70ce023703cc",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"https://i.kym-cdn.com/entries/icons/original/000/036/585/Attention_is_all_you_need.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b20477-0f01-4aba-93b5-ddeed3f68d1d",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "+ Python\n",
    "+ Working knowledge of Pytorch\n",
    "+ Neural Networks\n",
    "+ Seq2Seq\n",
    "+ Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d159f0f-1d3a-4995-86c1-dee8c61103d7",
   "metadata": {},
   "source": [
    "# Where did it come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a047d8-47b4-4673-a6de-4745aae5a473",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;RNN &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BERT\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;LSTM & GRU  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⬅ **2017 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)** ➡️ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GPT\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Attention &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f26276-1a93-4dad-8954-f7785d467fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c8ef1-904f-442d-848d-af4cdd2026e3",
   "metadata": {},
   "source": [
    "# The Architutre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660775d3-38ae-47b4-a3a0-1c8fac48e7d4",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:856/1*ZCFSvkKtppgew3cc7BIaug.png\" width=\"500px\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bba43-d9f8-4665-838f-952e8c594bcd",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c617f-6717-4fe9-b9cd-10778a7f1301",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1644eb5-9f2e-4a3a-ab7d-58c74e78bbef",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://i.imgur.com/thdHvQx.png\" width=\"400px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1044bd85-1e67-401d-9c5c-3d33a7724c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyAttention:\n",
    "    def __init__(self):\n",
    "        self.data = np.random.normal(size=(50,50))\n",
    "\n",
    "        self.weight_key = np.random.normal(size=(50,50))\n",
    "        self.weight_query = np.random.normal(size=(50,50))\n",
    "        self.weight_value = np.random.normal(size=(50,50))\n",
    "\n",
    "    def key(self):\n",
    "        return self.weight_key @ self.data\n",
    "\n",
    "    def query(self):\n",
    "        return self.weight_query @ self.data\n",
    "\n",
    "    def value(self):\n",
    "        return self.weight_value @ self.data\n",
    "\n",
    "    def forward(self):\n",
    "        k = self.key()\n",
    "        q = self.query()\n",
    "        v = self.value()\n",
    "\n",
    "        scores = k @ q.T\n",
    "        # scores = scores / k.shape[0]**0.5\n",
    "        scores = np.exp(scores) / np.sum(scores)\n",
    "        \n",
    "        \n",
    "        attn = scores @ v\n",
    "        return attn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0866376f-f9ae-4ff3-aafb-30ec77af7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/2dn04yws1xg1jbj6m8dsppgr0000gn/T/ipykernel_10633/1034976007.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  scores = np.exp(scores) / np.sum(scores)\n",
      "/var/folders/v7/2dn04yws1xg1jbj6m8dsppgr0000gn/T/ipykernel_10633/1034976007.py:28: RuntimeWarning: invalid value encountered in matmul\n",
      "  attn = scores @ v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ inf, -inf,  nan, ..., -inf,  nan,  inf],\n",
       "       ...,\n",
       "       [ nan,  nan,  inf, ...,  nan,  nan, -inf],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = DummyAttention()\n",
    "att = a.forward()\n",
    "print(att.shape)\n",
    "att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293eb63-e4a8-443e-9f14-82aaef21d748",
   "metadata": {},
   "source": [
    "## Multi-Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77bbe0-d470-4079-b4f7-51d33d9fe902",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1010/0*0KPEV8QidHkteKeY.png\" width=\"500px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf1eff9-1357-449b-b71c-fd04bf58f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Perform linear projections\n",
    "        Q = self.query(q)  # (batch_size, seq_length, d_model)\n",
    "        K = self.key(k)    # (batch_size, seq_length, d_model)\n",
    "        V = self.value(v)  # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Split the projections into multiple heads and reshape\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = torch.broadcast_to(mask.unsqueeze(1), (batch_size, self.num_heads, mask.shape[-1],mask.shape[-1]))\n",
    "            scores = scores.masked_fill(mask == 0, -1e20)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        \n",
    "        # Apply attention weights to the values\n",
    "        attention_output = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        \n",
    "        # Concatenate the heads and reshape\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.out(attention_output)  # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb3fbf-bd93-41ee-8c5a-72c8b397a35b",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d912a-5726-434c-a18e-aa3207b182e9",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54acabb8-5105-42ca-9ee1-e55488d35803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e7b43-c4e9-402e-bb77-1a16777af83a",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db779d91-f02d-4d0c-ad97-4eb3fcba982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922f52e-66a5-4298-ab35-3d9f8e340e56",
   "metadata": {},
   "source": [
    "## Embedding layer + Positional Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a7bf64-bb8c-409b-b7c1-7e59fa774983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self._generate_positional_encoding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_seq_len, d_model):\n",
    "        pos = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        i = torch.arange(d_model).unsqueeze(0)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
    "        pos_encoding = pos * angle_rates\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos_encoding[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos_encoding[:, 1::2])\n",
    "        return pos_encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7dd11-22ba-4524-bb63-2c37f0ba96b7",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41198f8d-1ce6-40df-8d31-2f79960df48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        \n",
    "        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, memory_mask)\n",
    "        x = self.norm2(x + self.dropout(enc_dec_attn_output))\n",
    "        \n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5f6e46-ac87-454d-a27b-835888dcfedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self._generate_positional_encoding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_seq_len, d_model):\n",
    "        pos = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        i = torch.arange(d_model).unsqueeze(0)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
    "        pos_encoding = pos * angle_rates\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos_encoding[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos_encoding[:, 1::2])\n",
    "        return pos_encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, tgt_mask, memory_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb0626-09d2-4f7b-a5c3-4649e60a26af",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1635c9-1f5d-4e75-8682-0adc397ec438",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13db8867-0bb9-408c-b4ae-8f283542da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, src_vocab_size, max_seq_len, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, tgt_vocab_size, max_seq_len, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, tgt_mask, memory_mask)\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2838a0-2c21-4c66-8a61-2ff355c60677",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daac7ab9-f5e2-4b97-9f7e-0c401e1476e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src, pad_idx):\n",
    "    src_mask = (src != pad_idx).unsqueeze(-2)\n",
    "    return src_mask\n",
    "\n",
    "def create_tgt_mask(tgt, pad_idx):\n",
    "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(-2)\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "    tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96a7928-6c80-4e42-aedb-0ac01ade2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "max_seq_len = 100\n",
    "dropout = 0.1\n",
    "pad_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11284d7-3a4e-46ff-9229-ddd59ca70d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100]) torch.Size([32, 100])\n",
      "torch.Size([32, 100, 10000])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout)\n",
    "src = torch.randint(0, src_vocab_size, (32, max_seq_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (32, max_seq_len))\n",
    "print(src.shape, tgt.shape)\n",
    "src_mask = create_src_mask(src, pad_idx)\n",
    "tgt_mask = create_tgt_mask(tgt, pad_idx)\n",
    "output = transformer(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89899149-4ae1-4d72-a37c-fbb582ca16dc",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011580-fa0a-477d-8ce8-41bb39d27f58",
   "metadata": {},
   "source": [
    "Step to convert text into words. \n",
    "\n",
    "Two simplest approches are:\n",
    "\n",
    "| Method              | Vocab Size | Sequence lengths |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Number for each char        |   Small   | Very long |\n",
    "| Number for each word           |   Very Large   | Contained (same as text) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71344487-196e-40d0-961d-c3aaa47edf26",
   "metadata": {},
   "source": [
    "Modern LLM use peicewise encoders, which are somewhere in between these two approches. Exact working is out of the scope of this lecture. Two popular approches are:-\n",
    "+ [Byte Pair Tokenization](https://www.youtube.com/watch?v=HEikzVL-lZU)\n",
    "+ [WordPiece Tokenization](https://www.youtube.com/watch?v=qpv6ms_t_1A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b4f2f-ce0b-41be-94d6-f3188960c467",
   "metadata": {},
   "source": [
    "# Why is transformer so revolutionary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660eb1d-f784-49ba-a190-44830b8fb83d",
   "metadata": {},
   "source": [
    "+ Very efficent compute wise\n",
    "+ [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) ==> Transformer is one of the greatest examples of this.\n",
    "+ Generalizable across domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59339592-4285-496d-a370-552d1493f58f",
   "metadata": {},
   "source": [
    "# Limitations of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ec5d6-a960-49e8-ab95-ab964edff18f",
   "metadata": {},
   "source": [
    "+ High memory usage\n",
    "+ Large compute and data requirements\n",
    "+ Limitations of token lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd10cd6-59e5-4640-8186-710fcedd7c5b",
   "metadata": {},
   "source": [
    "# How it leads to fancy stuff like BERT and GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccc21f-dfe4-445f-8f0d-c79700624347",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:434/1*D5xg0yz7YzBSzS_F1efLAA.png\" width=\"500px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3780c4-5582-43f0-8696-e77dced89480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
